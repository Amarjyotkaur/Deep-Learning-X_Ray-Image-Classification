{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a854606665b7be95b87ec3b093a413d97bb66a80afcd942ae742fb1176c8ada0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_classes import Lung_Train_Dataset, Lung_Test_Dataset, Lung_Val_Dataset\n",
    "# Torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is the training dataset of the Lung Dataset used for the Small Project in the 50.039 Deep Learning class in Feb-March 2021. \nIt contains a total of 5216 images, of size 150 by 150.\nThe images are stored in the following locations and each one contains the following number of images:\n - train_normal, in folder ./dataset/train/normal/: 1341 images.\n - train_infected(non_covid), in folder ./dataset/train/infected/non-covid/: 2530 images.\n - train_infected(covid), in folder ./dataset/train/infected/covid/: 1345 images.\n\n5216\ntorch.Size([1, 150, 150])\ntensor([[[0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         ...,\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\ntensor([0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "ld_train = Lung_Train_Dataset()\n",
    "ld_train.describe()\n",
    "print(len(ld_train))\n",
    "im, class_oh = ld_train[5215]\n",
    "print(im.shape)\n",
    "print(im)\n",
    "print(class_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is the test dataset of the Lung Dataset used for the Small Project in the 50.039 Deep Learning class in Feb-March 2021. \nIt contains a total of 615 images, of size 150 by 150.\nThe images are stored in the following locations and each one contains the following number of images:\n - test_normal, in folder ./dataset/test/normal/: 234 images.\n - test_infected(non_covid), in folder ./dataset/test/infected/non-covid/: 242 images.\n - test_infected(covid), in folder ./dataset/test/infected/covid/: 139 images.\n\n615\ntorch.Size([1, 150, 150])\ntensor([[[0.2588, 0.3725, 0.4314,  ..., 0.1176, 0.1255, 0.1333],\n         [0.3686, 0.4196, 0.4549,  ..., 0.1137, 0.1216, 0.1255],\n         [0.4392, 0.4275, 0.4510,  ..., 0.0941, 0.0941, 0.0941],\n         ...,\n         [0.1647, 0.1765, 0.1412,  ..., 0.1569, 0.1569, 0.1569],\n         [0.1647, 0.1725, 0.1373,  ..., 0.1569, 0.1569, 0.1569],\n         [0.1608, 0.1725, 0.1373,  ..., 0.1569, 0.1569, 0.1569]]])\ntensor([0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "ld_test = Lung_Test_Dataset()\n",
    "ld_test.describe()\n",
    "print(len(ld_test))\n",
    "im, class_oh = ld_test[476]\n",
    "print(im.shape)\n",
    "print(im)\n",
    "print(class_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is the validation dataset of the Lung Dataset used for the Small Project in the 50.039 Deep Learning class in Feb-March 2021. \nIt contains a total of 25 images, of size 150 by 150.\nThe images are stored in the following locations and each one contains the following number of images:\n - val_normal, in folder ./dataset/val/normal/: 8 images.\n - val_infected(non_covid), in folder ./dataset/val/infected/non-covid: 8 images.\n - val_infected(covid), in folder ./dataset/val/infected/covid: 9 images.\n\n25\ntorch.Size([1, 150, 150])\ntensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n         [0.0078, 0.0039, 0.0039,  ..., 0.0078, 0.0118, 0.0157],\n         ...,\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\ntensor([0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "ld_val = Lung_Val_Dataset()\n",
    "ld_val.describe()\n",
    "print(len(ld_val))\n",
    "im, class_oh = ld_val[15]\n",
    "print(im.shape)\n",
    "print(im)\n",
    "print(class_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_val = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001D4C94C89C8>\n<torch.utils.data.dataloader.DataLoader object at 0x000001D4CCB6ED88>\n<torch.utils.data.dataloader.DataLoader object at 0x000001D4CCB65248>\n"
     ]
    }
   ],
   "source": [
    "# creating dataloader object\n",
    "train_loader = DataLoader(ld_train, batch_size = bs_val, shuffle = True)\n",
    "print(train_loader)\n",
    "test_loader = DataLoader(ld_test, batch_size = bs_val, shuffle = True)\n",
    "print(test_loader)\n",
    "val_loader = DataLoader(ld_val, batch_size = bs_val, shuffle = True)\n",
    "print(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----\n",
      "0\n",
      "tensor([[[[0.2196, 0.2784, 0.2863,  ..., 0.1608, 0.1451, 0.1333],\n",
      "          [0.2196, 0.2667, 0.2745,  ..., 0.1529, 0.1333, 0.1255],\n",
      "          [0.2235, 0.2510, 0.2667,  ..., 0.1451, 0.1255, 0.1137],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0078, 0.0549, 0.1686,  ..., 0.0353, 0.0078, 0.0000],\n",
      "          [0.0235, 0.0588, 0.1529,  ..., 0.0353, 0.0039, 0.0000],\n",
      "          [0.0235, 0.0627, 0.1529,  ..., 0.0353, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0588, 0.0902, 0.1333,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0471, 0.0784, 0.1255,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0353, 0.0706, 0.1137,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0196, 0.0353, 0.0000,  ..., 0.2000, 0.1255, 0.1490],\n",
      "          [0.0078, 0.0314, 0.0078,  ..., 0.0980, 0.1922, 0.1922],\n",
      "          [0.0000, 0.0078, 0.0275,  ..., 0.1686, 0.2471, 0.2000],\n",
      "          ...,\n",
      "          [0.0471, 0.0471, 0.0667,  ..., 0.0039, 0.0039, 0.0039],\n",
      "          [0.0078, 0.0157, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
      "          [0.1725, 0.1882, 0.0745,  ..., 0.0039, 0.0039, 0.0039]]],\n",
      "\n",
      "\n",
      "        [[[0.0980, 0.1059, 0.1216,  ..., 0.1569, 0.1569, 0.1608],\n",
      "          [0.1059, 0.1176, 0.1294,  ..., 0.2039, 0.2039, 0.2000],\n",
      "          [0.1255, 0.1333, 0.1529,  ..., 0.2118, 0.2118, 0.2157],\n",
      "          ...,\n",
      "          [0.0157, 0.0078, 0.0078,  ..., 0.0078, 0.0157, 0.0196],\n",
      "          [0.0039, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0078],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],\n",
      "          [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],\n",
      "          [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Typical mini-batch for loop on dataloader (train)\n",
    "for k, v in enumerate(train_loader):\n",
    "    print(\"-----\")\n",
    "    print(k)\n",
    "    print(v[0])\n",
    "    print(v[1])\n",
    "    # Forced stop\n",
    "    break\n",
    "    #assert False, \"Forced stop after one iteration of the for loop\""
   ]
  },
  {
   "source": [
    "Testing of the dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple mode\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Conv2D: 1 input channel, 8 output channels, 3 by 3 kernel, stride of 1.\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
    "        self.fc1 = nn.Linear(87616, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim = 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.6467, -0.7419],\n        [-0.6536, -0.7344],\n        [-0.6363, -0.7534],\n        [-0.6019, -0.7936],\n        [-0.5785, -0.8227],\n        [-0.5598, -0.8471],\n        [-0.5923, -0.8053],\n        [-0.6432, -0.7457]], grad_fn=<LogSoftmaxBackward>)\ntensor([[0., 0., 1.],\n        [0., 0., 1.],\n        [1., 0., 0.],\n        [0., 1., 0.],\n        [0., 1., 0.],\n        [0., 1., 0.],\n        [1., 0., 0.],\n        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = Net()\n",
    "# Try model on one mini-batch\n",
    "for batch_idx, (images_data, target_labels) in enumerate(train_loader):\n",
    "    predicted_labels = model(images_data)\n",
    "    print(predicted_labels)\n",
    "    print(target_labels)\n",
    "    # Forced stop\n",
    "    break\n",
    "    #assert False, \"Forced stop after one iteration of the mini-batch for loop\""
   ]
  }
 ]
}